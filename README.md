# ZyzyvaTelemetry

A lightweight monitoring library for distributed Elixir applications that provides local-first error logging, health reporting, and distributed tracing without network blocking.

## Features

- **Local-first monitoring** - Writes to shared SQLite database at `/var/lib/monitoring/events.db`
- **Zero network blocking** - All writes are local file operations
- **Automatic health reporting** - Configurable periodic health checks
- **Correlation ID tracking** - Follow requests across multiple services
- **OTP supervision** - Proper supervision tree integration via MonitoringSupervisor
- **Broadway monitoring** - Built-in RabbitMQ/Broadway pipeline health checks
- **Minimal dependencies** - Only requires `exqlite` and `plug`, uses Elixir 1.18+ native JSON

## What This Library Monitors

### ✅ Automatically Monitored (every 30 seconds)

- **Memory usage** - With automatic status thresholds (healthy/degraded/unhealthy)
- **Process count** - Total BEAM processes
- **Database connectivity** - Simple `SELECT 1` health check if repo configured
- **Broadway/RabbitMQ status** - Checks if Broadway pipelines are running
- **Custom health checks** - Any additional checks you provide

### ⚠️ Requires Manual Instrumentation

The following require explicit calls to logging functions:

- **Application errors** - Must call `ZyzyvaTelemetry.log_error()`
- **Warnings** - Must call `ZyzyvaTelemetry.log_warning()`
- **Exceptions** - Must catch and call `ZyzyvaTelemetry.log_exception()`

### ❌ NOT Automatically Monitored

This library does **not** automatically monitor:

- **Database query performance** - No automatic slow query detection
- **HTTP request latencies** - No automatic endpoint monitoring
- **Phoenix metrics** - No automatic controller/LiveView tracking
- **GenServer crashes** - No automatic process crash detection
- **Process mailbox sizes** - No automatic mailbox monitoring
- **ETS table metrics** - No automatic table size tracking
- **System metrics** - No CPU, disk I/O, or network monitoring

To monitor these, you would need to:
1. Attach to the relevant `:telemetry` events
2. Explicitly call `ZyzyvaTelemetry.log_error()` or `log_warning()` when thresholds are exceeded

### Example: Adding Slow Query Monitoring

To actually monitor slow database queries (not included by default):

```elixir
# In your application.ex start/2 function
def start(_type, _args) do
  # ... existing code ...
  
  # Attach to Ecto telemetry events
  :telemetry.attach(
    "ecto-slow-queries",
    [:my_app, :repo, :query],
    &handle_slow_queries/4,
    %{threshold_ms: 500}  # Log queries slower than 500ms
  )
end

defp handle_slow_queries(_event_name, measurements, metadata, config) do
  duration_ms = measurements.total_time / 1_000_000
  
  if duration_ms > config.threshold_ms do
    ZyzyvaTelemetry.log_warning("Slow query detected", %{
      query: metadata.query,
      duration_ms: round(duration_ms),
      source: metadata.source
    })
  end
end
```

**Note**: The test events generated by `generate_test_events()` may include "slow_query" events, but these are **simulated** for testing purposes, not based on actual query measurements.

## Requirements

- Elixir 1.18+ (for native JSON support)
- Write access to `/var/lib/monitoring/` or configured database path

## Installation

Add `zyzyva_telemetry` to your list of dependencies in `mix.exs`:

```elixir
def deps do
  [
    {:zyzyva_telemetry, "~> 0.1.0"}
  ]
end
```

### Database Setup

The monitoring database needs to be created at `/var/lib/monitoring/events.db`. This typically requires elevated permissions.

#### From Development

```bash
# Using the included setup module
mix run -e "ZyzyvaTelemetry.Setup.init()"
```

#### From a Release

```bash
# The setup module is available in your release
./my_app eval "ZyzyvaTelemetry.Setup.init()"

# Or with a custom path
./my_app eval "ZyzyvaTelemetry.Setup.init('/custom/path/events.db')"
```

#### Manual Setup

If you prefer to set up the directory manually:

```bash
sudo mkdir -p /var/lib/monitoring
sudo chown $USER:$USER /var/lib/monitoring
sudo chmod 755 /var/lib/monitoring
```

The database file and tables will be created automatically when the application starts.

## Usage

### Quick Setup for Phoenix Apps

Run the setup task for automated integration:

```bash
mix zyzyva.setup
```

This comprehensive setup task will:
- Generate a test helper module for health endpoint testing
- Automatically add test configuration to `config/test.exs`
- Create a temporary test database configuration
- Provide complete integration code snippets for your application
- Show Broadway pipeline configuration examples (if applicable)

The task provides copy-paste ready code for:
1. Adding MonitoringSupervisor to your application supervision tree
2. Configuring Broadway pipelines in your config files
3. Adding correlation tracking to your browser pipeline
4. Setting up the health endpoint with proper routing
5. Creating comprehensive health endpoint tests

### Integration

The MonitoringSupervisor is the primary integration point. Add it to your application's supervision tree:

```elixir
# In lib/my_app/application.ex
def start(_type, _args) do
  children = [
    # ... your existing children (Repo, PubSub, etc.)
    
    # Add the monitoring supervisor
    {ZyzyvaTelemetry.MonitoringSupervisor,
     service_name: "my_app",
     repo: MyApp.Repo,  # Optional: for database health checks
     broadway_pipelines: Application.get_env(:my_app, :broadway_pipelines, [])}
  ]
  
  opts = [strategy: :one_for_one, name: MyApp.Supervisor]
  Supervisor.start_link(children, opts)
end
```

#### Configuration-Based Broadway Monitoring

For cleaner separation of concerns, configure Broadway pipelines in your config files:

```elixir
# config/config.exs
config :my_app,
  broadway_pipelines: [
    MyApp.Pipeline.Broadway,
    MyApp.AnotherPipeline.Broadway
  ]

# config/test.exs - Override for test environment
config :my_app,
  broadway_pipelines: []  # No Broadway pipelines in test
```

This approach keeps your application.ex clean and makes it easy to configure different pipelines per environment.

### Logging Errors

```elixir
# Simple error
ZyzyvaTelemetry.log_error("Something went wrong")

# Error with metadata
ZyzyvaTelemetry.log_error("Failed to process", %{user_id: 123, action: "create"})

# Warning
ZyzyvaTelemetry.log_warning("Memory usage high")

# Exception with stack trace
try do
  risky_operation()
rescue
  e ->
    ZyzyvaTelemetry.log_exception(e, __STACKTRACE__, "Operation failed")
end
```

### Correlation Tracking

For Phoenix applications, add the correlation plug to your pipeline:

```elixir
pipeline :browser do
  # ... other plugs
  plug ZyzyvaTelemetry.Plugs.CorrelationTracker
end

# Add health endpoint - IMPORTANT: HealthController is a Plug, not a Phoenix controller
# Use Plug syntax (not controller syntax):
get "/health", ZyzyvaTelemetry.HealthController, []
```

Track requests programmatically:

```elixir
# Set correlation ID for distributed tracing
ZyzyvaTelemetry.with_correlation(request_id, fn ->
  # All errors logged here will include the correlation ID
  process_request()
end)

# Or manually manage correlation
ZyzyvaTelemetry.set_correlation("request-123")
ZyzyvaTelemetry.log_error("Failed") # Will include correlation ID
```

### Broadway/RabbitMQ Monitoring

Broadway pipeline monitoring is built into the MonitoringSupervisor. When you provide the `broadway_pipelines` option, the health reporter will automatically:

- Check if each Broadway pipeline process is running
- Report `rabbitmq_connected: true/false` in health checks
- Include Broadway status in overall health determination

The monitoring works by checking if the named Broadway processes are alive, which indicates RabbitMQ connectivity.

### Health Reporting

The health reporter runs automatically at configured intervals (default: 30 seconds). Health data includes:

- Memory usage with automatic status thresholds
- Process count monitoring
- Database connectivity (if repo configured)
- Broadway/RabbitMQ status (if pipelines configured)
- Custom health checks (if provided)

You can also report health manually:

```elixir
# Manual health report
ZyzyvaTelemetry.report_health(%{
  status: :degraded,
  reason: "High memory usage",
  memory_mb: 1024
})
```

#### Custom Health Checks

Add custom health checks via the `extra_health_checks` option:

```elixir
{ZyzyvaTelemetry.MonitoringSupervisor,
 service_name: "my_app",
 repo: MyApp.Repo,
 extra_health_checks: %{
   queue_depth: fn -> %{queue_depth: MyApp.Queue.depth()} end,
   cache_status: fn -> %{cache_hit_rate: MyApp.Cache.hit_rate()} end
 }}
```

## Testing and QA

### Test Event Generation

ZyzyvaTelemetry includes built-in test event generators for QA and testing purposes. These functions are available at runtime in production releases, making it easy to verify your monitoring pipeline end-to-end.

#### Generating Test Events

```elixir
# Generate 10 test events (default)
ZyzyvaTelemetry.generate_test_events()

# Generate 50 events with various severities
ZyzyvaTelemetry.generate_test_events(count: 50, include_errors: true)

# Generate events including critical alerts
ZyzyvaTelemetry.generate_test_events(count: 20, include_critical: true)

# Use custom service name (defaults to app name)
ZyzyvaTelemetry.generate_test_events(service_name: "my_custom_service")
```

#### Simulating Critical Incidents

Test your alerting system by generating a critical incident scenario:

```elixir
# Generate escalating events: warnings → errors → critical
ZyzyvaTelemetry.generate_critical_incident()

# Generate for specific service
ZyzyvaTelemetry.generate_critical_incident("api_gateway")
```

This will generate a realistic incident pattern:
- Initial warning events (memory spikes, slow queries)
- Escalating to errors (timeouts, connection errors)
- Culminating in critical event (service unavailable)

#### Performance Degradation Testing

Simulate gradual performance degradation:

```elixir
# Run 30-second performance degradation test (default)
ZyzyvaTelemetry.generate_performance_degradation()

# Run 60-second test
ZyzyvaTelemetry.generate_performance_degradation(60)
```

This generates increasingly slow query events over the specified duration.

### Using the Mix Task

For development environments, you can use the mix task:

```bash
# Generate 10 normal test events
mix zyzyva.test_events

# Generate 50 events including critical ones
mix zyzyva.test_events --count 50 --critical

# Generate a critical incident
mix zyzyva.test_events --incident

# Run a 60-second performance degradation test
mix zyzyva.test_events --performance 60

# Specify custom service name
mix zyzyva.test_events --service my_service --count 20
```

### Testing in Production

Since the test generators are available at runtime, you can test monitoring in production environments:

1. **Remote Console Testing**:
```bash
# Attach to running node
./my_app remote

# In the console
ZyzyvaTelemetry.generate_test_events(count: 5, service_name: "prod_test")
```

2. **Eval Testing** (without attaching):
```bash
./my_app eval 'ZyzyvaTelemetry.generate_test_events(count: 5)'
```

3. **Custom Endpoint** (optional):
You could add an admin endpoint that triggers test events:
```elixir
def test_monitoring(conn, _params) do
  {:ok, summary} = ZyzyvaTelemetry.generate_test_events(count: 10)
  json(conn, %{status: "ok", events_generated: summary.successful})
end
```

### Verifying the Monitoring Pipeline

Test events include special metadata to identify them:
- `test_event: true` - Marks events as test-generated
- `test_batch_id` - Groups related test events
- `test_sequence` - Order within the batch

This allows your monitoring system to:
1. Filter test events from production metrics if needed
2. Verify end-to-end pipeline functionality
3. Test alerting thresholds without affecting real metrics

### Unit Testing

The setup task generates a test helper module and configures your test environment automatically. The test helper provides reusable assertions for testing health endpoints.

Example usage:
```elixir
defmodule MyAppWeb.HealthControllerTest do
  use MyAppWeb.ConnCase
  import MyAppWeb.HealthEndpointTestHelper

  test "GET /health returns telemetry data", %{conn: conn} do
    conn = get(conn, "/health")
    body = assert_health_endpoint(conn,
      service_name: "my_app",
      required_fields: ["memory", "processes", "database_connected"]
    )
  end
end
```

### Integration Testing

For testing the full monitoring pipeline:

1. **Generate test events** using the functions above
2. **Wait for aggregation** (typically 30-60 seconds)
3. **Verify in monitoring dashboard** that events appear with correct:
   - Service name and health status
   - Server hostname and node information
   - Event counts and severities
   - Alert triggering for critical events

## Configuration Options

The MonitoringSupervisor accepts these options:

- `service_name` - Application name (required, defaults to Mix.Project app name)
- `repo` - Ecto repo module for database health checks (optional)
- `broadway_pipelines` - List of Broadway pipeline modules to monitor (optional)
- `extra_health_checks` - Map of custom health check functions (optional)
- `health_interval_ms` - Health check interval in milliseconds (default: 30_000)
- `db_path` - Database path (default: `/var/lib/monitoring/events.db`)
- `node_id` - Node identifier (defaults to `node()`)

## Data Management

The library writes events to a shared SQLite database that grows over time. Data retention and cleanup should be handled by a separate aggregator service (not included) that:

1. Reads unforwarded events from the database
2. Forwards them to central monitoring
3. Marks events as forwarded using `SqliteWriter.mark_events_forwarded/2`
4. Deletes old forwarded events periodically
5. Runs VACUUM to reclaim disk space

Without an aggregator, expect approximately:
- 10 services: ~24 MB/day
- 50 services: ~120 MB/day
- 100 services: ~240 MB/day

## Architecture

ZyzyvaTelemetry uses a local-first architecture with these key components:

- **MonitoringSupervisor** - Main OTP supervisor that manages all monitoring processes
- **HealthReporter** - GenServer that periodically collects and reports health metrics
- **ErrorLogger** - Centralized error and warning logging with correlation support
- **SqliteWriter** - Direct SQLite operations with support for forwarding markers
- **Correlation** - Process-dictionary based correlation ID tracking

The library writes all monitoring data to a local SQLite database that is shared by all applications on the server. A separate aggregator service (not included) can forward this data to a central monitoring system.

### Database Performance

The SQLite database at `/var/lib/monitoring/events.db` is optimized for concurrent access:

- **WAL mode** - Write-Ahead Logging for better concurrency (readers don't block writers)
- **NORMAL synchronous** - Safe with WAL, better performance than FULL
- **10MB cache** - Reduced disk I/O for frequently accessed data
- **Memory-mapped I/O** - Up to 256MB for faster access
- **Proper indexes** - Efficient queries on forwarded status, timestamps, and correlation IDs

